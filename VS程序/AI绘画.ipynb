{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a6f61899bed3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m \u001b[0mcartoonize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-a6f61899bed3>\u001b[0m in \u001b[0;36mcartoonize\u001b[1;34m(load_folder, save_folder, model_path)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;31m#（2）卡通化函数定义：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcartoonize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m     \u001b[0minput_photo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[0mnetwork_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munet_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_photo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[0mfinal_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mguided_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mguided_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_photo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def resblock(inputs, out_channel=32, name='resblock'):\n",
    "    with tf.variable_scope(name):\n",
    "        x = slim.convolution2d(inputs, out_channel, [3, 3], \n",
    "                               activation_fn=None, scope='conv1')\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = slim.convolution2d(x, out_channel, [3, 3], \n",
    "                               activation_fn=None, scope='conv2')\n",
    "        return x + inputs\n",
    "\n",
    "    \n",
    "def generator(inputs, channel=32, num_blocks=4, name='generator', reuse=False):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        x = slim.convolution2d(inputs, channel, [7, 7], activation_fn=None)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = slim.convolution2d(x, channel*2, [3, 3], stride=2, activation_fn=None)\n",
    "        x = slim.convolution2d(x, channel*2, [3, 3], activation_fn=None)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = slim.convolution2d(x, channel*4, [3, 3], stride=2, activation_fn=None)\n",
    "        x = slim.convolution2d(x, channel*4, [3, 3], activation_fn=None)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        for idx in range(num_blocks):\n",
    "            x = resblock(x, out_channel=channel*4, name='block_{}'.format(idx))\n",
    "        x = slim.conv2d_transpose(x, channel*2, [3, 3], stride=2, activation_fn=None)\n",
    "        x = slim.convolution2d(x, channel*2, [3, 3], activation_fn=None)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = slim.conv2d_transpose(x, channel, [3, 3], stride=2, activation_fn=None)\n",
    "        x = slim.convolution2d(x, channel, [3, 3], activation_fn=None)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = slim.convolution2d(x, 3, [7, 7], activation_fn=None)\n",
    "        #x = tf.clip_by_value(x, -0.999999, 0.999999)\n",
    "        return x\n",
    "def unet_generator(inputs, channel=32, num_blocks=4, name='generator', reuse=False):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        x0 = slim.convolution2d(inputs, channel, [7, 7], activation_fn=None)\n",
    "        x0 = tf.nn.leaky_relu(x0)\n",
    "        x1 = slim.convolution2d(x0, channel, [3, 3], stride=2, activation_fn=None)\n",
    "        x1 = tf.nn.leaky_relu(x1)\n",
    "        x1 = slim.convolution2d(x1, channel*2, [3, 3], activation_fn=None)\n",
    "        x1 = tf.nn.leaky_relu(x1)\n",
    "        x2 = slim.convolution2d(x1, channel*2, [3, 3], stride=2, activation_fn=None)\n",
    "        x2 = tf.nn.leaky_relu(x2)\n",
    "        x2 = slim.convolution2d(x2, channel*4, [3, 3], activation_fn=None)\n",
    "        x2 = tf.nn.leaky_relu(x2)\n",
    "        for idx in range(num_blocks):\n",
    "            x2 = resblock(x2, out_channel=channel*4, name='block_{}'.format(idx))\n",
    "        x2 = slim.convolution2d(x2, channel*2, [3, 3], activation_fn=None)\n",
    "        x2 = tf.nn.leaky_relu(x2)\n",
    "        h1, w1 = tf.shape(x2)[1], tf.shape(x2)[2]\n",
    "        x3 = tf.image.resize_bilinear(x2, (h1*2, w1*2))\n",
    "        x3 = slim.convolution2d(x3+x1, channel*2, [3, 3], activation_fn=None)\n",
    "        x3 = tf.nn.leaky_relu(x3)\n",
    "        x3 = slim.convolution2d(x3, channel, [3, 3], activation_fn=None)\n",
    "        x3 = tf.nn.leaky_relu(x3)\n",
    "        h2, w2 = tf.shape(x3)[1], tf.shape(x3)[2]\n",
    "        x4 = tf.image.resize_bilinear(x3, (h2*2, w2*2))\n",
    "        x4 = slim.convolution2d(x4+x0, channel, [3, 3], activation_fn=None)\n",
    "        x4 = tf.nn.leaky_relu(x4)\n",
    "        x4 = slim.convolution2d(x4, 3, [7, 7], activation_fn=None)\n",
    "        #x4 = tf.clip_by_value(x4, -1, 1)\n",
    "        return x4\n",
    "\n",
    "def disc_bn(x, scale=1, channel=32, is_training=True, \n",
    "            name='discriminator', patch=True, reuse=False):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        for idx in range(3):\n",
    "            x = slim.convolution2d(x, channel*2**idx, [3, 3], stride=2, activation_fn=None)\n",
    "            x = slim.batch_norm(x, is_training=is_training, center=True, scale=True)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            x = slim.convolution2d(x, channel*2**idx, [3, 3], activation_fn=None)\n",
    "            x = slim.batch_norm(x, is_training=is_training, center=True, scale=True)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "        if patch == True:\n",
    "            x = slim.convolution2d(x, 1, [1, 1], activation_fn=None)\n",
    "        else:\n",
    "            x = tf.reduce_mean(x, axis=[1, 2])\n",
    "            x = slim.fully_connected(x, 1, activation_fn=None)\n",
    "        return x\n",
    "    \n",
    "def disc_sn(x, scale=1, channel=32, patch=True, name='discriminator', reuse=False):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        for idx in range(3):\n",
    "            x = layers.conv_spectral_norm(x, channel*2**idx, [3, 3], \n",
    "                                          stride=2, name='conv{}_1'.format(idx))\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            x = layers.conv_spectral_norm(x, channel*2**idx, [3, 3], \n",
    "                                          name='conv{}_2'.format(idx))\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "        if patch == True:\n",
    "            x = layers.conv_spectral_norm(x, 1, [1, 1], name='conv_out'.format(idx))\n",
    "        else:\n",
    "            x = tf.reduce_mean(x, axis=[1, 2])\n",
    "            x = slim.fully_connected(x, 1, activation_fn=None)\n",
    "        return x\n",
    "    \n",
    "def disc_ln(x, channel=32, is_training=True, name='discriminator', patch=True, reuse=False):\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        for idx in range(3):\n",
    "            x = slim.convolution2d(x, channel*2**idx, [3, 3], stride=2, activation_fn=None)\n",
    "            x = tf.contrib.layers.layer_norm(x)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            x = slim.convolution2d(x, channel*2**idx, [3, 3], activation_fn=None)\n",
    "            x = tf.contrib.layers.layer_norm(x)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "        if patch == True:\n",
    "            x = slim.convolution2d(x, 1, [1, 1], activation_fn=None)\n",
    "        else:\n",
    "            x = tf.reduce_mean(x, axis=[1, 2])\n",
    "            x = slim.fully_connected(x, 1, activation_fn=None)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def train(args):\n",
    "    input_photo = tf.placeholder(tf.float32, [args.batch_size, \n",
    "                                args.patch_size, args.patch_size, 3])\n",
    "    input_superpixel = tf.placeholder(tf.float32, [args.batch_size, \n",
    "                                args.patch_size, args.patch_size, 3])\n",
    "    input_cartoon = tf.placeholder(tf.float32, [args.batch_size, \n",
    "                                args.patch_size, args.patch_size, 3])\n",
    "    output = network.unet_generator(input_photo)\n",
    "    output = guided_filter(input_photo, output, r=1)\n",
    "    blur_fake = guided_filter(output, output, r=5, eps=2e-1)\n",
    "    blur_cartoon = guided_filter(input_cartoon, input_cartoon, r=5, eps=2e-1)\n",
    "    gray_fake, gray_cartoon = utils.color_shift(output, input_cartoon)\n",
    "    d_loss_gray, g_loss_gray = loss.lsgan_loss(network.disc_sn, gray_cartoon, gray_fake, \n",
    "                                             scale=1, patch=True, name='disc_gray')\n",
    "    d_loss_blur, g_loss_blur = loss.lsgan_loss(network.disc_sn, blur_cartoon, blur_fake, \n",
    "                                             scale=1, patch=True, name='disc_blur')\n",
    "    vgg_model = loss.Vgg19('vgg19_no_fc.npy')\n",
    "    vgg_photo = vgg_model.build_conv4_4(input_photo)\n",
    "    vgg_output = vgg_model.build_conv4_4(output)\n",
    "    vgg_superpixel = vgg_model.build_conv4_4(input_superpixel)\n",
    "    h, w, c = vgg_photo.get_shape().as_list()[1:]\n",
    "    photo_loss = tf.reduce_mean(tf.losses.absolute_difference(vgg_photo, vgg_output))/(h*w*c)\n",
    "    superpixel_loss = tf.reduce_mean(tf.losses.absolute_difference\\\n",
    "                                     (vgg_superpixel, vgg_output))/(h*w*c)\n",
    "    recon_loss = photo_loss + superpixel_loss\n",
    "    tv_loss = loss.total_variation_loss(output)\n",
    "    g_loss_total = 1e4*tv_loss + 1e-1*g_loss_blur + g_loss_gray + 2e2*recon_loss\n",
    "    d_loss_total = d_loss_blur + d_loss_gray\n",
    "    all_vars = tf.trainable_variables()\n",
    "    gene_vars = [var for var in all_vars if 'gene' in var.name]\n",
    "    disc_vars = [var for var in all_vars if 'disc' in var.name] \n",
    "    tf.summary.scalar('tv_loss', tv_loss)\n",
    "    tf.summary.scalar('photo_loss', photo_loss)\n",
    "    tf.summary.scalar('superpixel_loss', superpixel_loss)\n",
    "    tf.summary.scalar('recon_loss', recon_loss)\n",
    "    tf.summary.scalar('d_loss_gray', d_loss_gray)\n",
    "    tf.summary.scalar('g_loss_gray', g_loss_gray)\n",
    "    tf.summary.scalar('d_loss_blur', d_loss_blur)\n",
    "    tf.summary.scalar('g_loss_blur', g_loss_blur)\n",
    "    tf.summary.scalar('d_loss_total', d_loss_total)\n",
    "    tf.summary.scalar('g_loss_total', g_loss_total)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        g_optim = tf.train.AdamOptimizer(args.adv_train_lr, beta1=0.5, beta2=0.99)\\\n",
    "                                        .minimize(g_loss_total, var_list=gene_vars)\n",
    "        d_optim = tf.train.AdamOptimizer(args.adv_train_lr, beta1=0.5, beta2=0.99)\\\n",
    "                                        .minimize(d_loss_total, var_list=disc_vars)\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_fraction)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    train_writer = tf.summary.FileWriter(args.save_dir+'/train_log')\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver(var_list=gene_vars, max_to_keep=20)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('pretrain/saved_models'))\n",
    "        face_photo_dir = 'dataset/photo_face'\n",
    "        face_photo_list = utils.load_image_list(face_photo_dir)\n",
    "        scenery_photo_dir = 'dataset/photo_scenery'\n",
    "        scenery_photo_list = utils.load_image_list(scenery_photo_dir)\n",
    "        face_cartoon_dir = 'dataset/cartoon_face'\n",
    "        face_cartoon_list = utils.load_image_list(face_cartoon_dir)\n",
    "        scenery_cartoon_dir = 'dataset/cartoon_scenery'\n",
    "        scenery_cartoon_list = utils.load_image_list(scenery_cartoon_dir)\n",
    "        for total_iter in tqdm(range(args.total_iter)):\n",
    "            if np.mod(total_iter, 5) == 0: \n",
    "                photo_batch = utils.next_batch(face_photo_list, args.batch_size)\n",
    "                cartoon_batch = utils.next_batch(face_cartoon_list, args.batch_size)\n",
    "            else:\n",
    "                photo_batch = utils.next_batch(scenery_photo_list, args.batch_size)\n",
    "                cartoon_batch = utils.next_batch(scenery_cartoon_list, args.batch_size)\n",
    "            inter_out = sess.run(output, feed_dict={input_photo: photo_batch, \n",
    "                                                    input_superpixel: photo_batch,\n",
    "                                                    input_cartoon: cartoon_batch})\n",
    "            if args.use_enhance:\n",
    "                superpixel_batch = utils.selective_adacolor(inter_out, power=1.2)\n",
    "            else:\n",
    "                superpixel_batch = utils.simple_superpixel(inter_out, seg_num=200)\n",
    "            _, g_loss, r_loss = sess.run([g_optim, g_loss_total, recon_loss],  \n",
    "                                            feed_dict={input_photo: photo_batch, \n",
    "                                                    input_superpixel: superpixel_batch,\n",
    "                                                    input_cartoon: cartoon_batch})\n",
    "            _, d_loss, train_info = sess.run([d_optim, d_loss_total, summary_op],  \n",
    "                                            feed_dict={input_photo: photo_batch, \n",
    "                                                    input_superpixel: superpixel_batch,\n",
    "                                                    input_cartoon: cartoon_batch})\n",
    "            train_writer.add_summary(train_info, total_iter)\n",
    "            if np.mod(total_iter+1, 50) == 0:\n",
    "                print('Iter: {}, d_loss: {}, g_loss: {}, recon_loss: {}'.\\\n",
    "                        format(total_iter, d_loss, g_loss, r_loss))\n",
    "                if np.mod(total_iter+1, 500 ) == 0:\n",
    "                    saver.save(sess, args.save_dir+'/saved_models/model', \n",
    "                               write_meta_graph=False, global_step=total_iter)\n",
    "                    photo_face = utils.next_batch(face_photo_list, args.batch_size)\n",
    "                    cartoon_face = utils.next_batch(face_cartoon_list, args.batch_size)\n",
    "                    photo_scenery = utils.next_batch(scenery_photo_list, args.batch_size)\n",
    "                    cartoon_scenery = utils.next_batch(scenery_cartoon_list, args.batch_size)\n",
    "                    result_face = sess.run(output, feed_dict={input_photo: photo_face, \n",
    "                                                            input_superpixel: photo_face,\n",
    "                                                            input_cartoon: cartoon_face})\n",
    "                    result_scenery = sess.run(output, feed_dict={input_photo: photo_scenery, \n",
    "                                                                input_superpixel: photo_scenery,\n",
    "                                                                input_cartoon: cartoon_scenery})\n",
    "                    utils.write_batch_image(result_face, args.save_dir+'/images', \n",
    "                                            str(total_iter)+'_face_result.jpg', 4)\n",
    "                    utils.write_batch_image(photo_face, args.save_dir+'/images', \n",
    "                                            str(total_iter)+'_face_photo.jpg', 4)\n",
    "                    utils.write_batch_image(result_scenery, args.save_dir+'/images', \n",
    "                                            str(total_iter)+'_scenery_result.jpg', 4)\n",
    "                    utils.write_batch_image(photo_scenery, args.save_dir+'/images', \n",
    "                                            str(total_iter)+'_scenery_photo.jpg', 4)\n",
    "\n",
    "def resize_crop(image):\n",
    "    h, w, c = np.shape(image)\n",
    "    if min(h, w) > 720:\n",
    "        if h > w:\n",
    "            h, w = int(720*h/w), 720\n",
    "        else:\n",
    "            h, w = 720, int(720*w/h)\n",
    "    image = cv2.resize(image, (w, h),\n",
    "                       interpolation=cv2.INTER_AREA)\n",
    "    h, w = (h//8)*8, (w//8)*8\n",
    "    image = image[:h, :w, :]\n",
    "    return image\n",
    "def tf_box_filter(x, r):\n",
    "    k_size = int(2*r+1)\n",
    "    ch = x.get_shape().as_list()[-1]\n",
    "    weight = 1/(k_size**2)\n",
    "    box_kernel = weight*np.ones((k_size, k_size, ch, 1))\n",
    "    box_kernel = np.array(box_kernel).astype(np.float32)\n",
    "    output = tf.nn.depthwise_conv2d(x, box_kernel, [1, 1, 1, 1], 'SAME')\n",
    "    return output\n",
    "def guided_filter(x, y, r, eps=1e-2):\n",
    "    x_shape = tf.shape(x)\n",
    "    #y_shape = tf.shape(y)\n",
    "    N = tf_box_filter(tf.ones((1, x_shape[1], x_shape[2], 1), dtype=x.dtype), r)\n",
    "    mean_x = tf_box_filter(x, r) / N\n",
    "    mean_y = tf_box_filter(y, r) / N\n",
    "    cov_xy = tf_box_filter(x * y, r) / N - mean_x * mean_y\n",
    "    var_x  = tf_box_filter(x * x, r) / N - mean_x * mean_x\n",
    "    A = cov_xy / (var_x + eps)\n",
    "    b = mean_y - A * mean_x\n",
    "    mean_A = tf_box_filter(A, r) / N\n",
    "    mean_b = tf_box_filter(b, r) / N\n",
    "    output = mean_A * x + mean_b\n",
    "    return output\n",
    "\n",
    "def fast_guided_filter(lr_x, lr_y, hr_x, r=1, eps=1e-8):\n",
    "    #assert lr_x.shape.ndims == 4 and lr_y.shape.ndims == 4 and hr_x.shape.ndims == 4\n",
    "    lr_x_shape = tf.shape(lr_x)\n",
    "    #lr_y_shape = tf.shape(lr_y)\n",
    "    hr_x_shape = tf.shape(hr_x)\n",
    "    N = tf_box_filter(tf.ones((1, lr_x_shape[1], lr_x_shape[2], 1), dtype=lr_x.dtype), r)\n",
    "    mean_x = tf_box_filter(lr_x, r) / N\n",
    "    mean_y = tf_box_filter(lr_y, r) / N\n",
    "    cov_xy = tf_box_filter(lr_x * lr_y, r) / N - mean_x * mean_y\n",
    "    var_x  = tf_box_filter(lr_x * lr_x, r) / N - mean_x * mean_x\n",
    "    A = cov_xy / (var_x + eps)\n",
    "    b = mean_y - A * mean_x\n",
    "    mean_A = tf.image.resize_images(A, hr_x_shape[1: 3])\n",
    "    mean_b = tf.image.resize_images(b, hr_x_shape[1: 3])\n",
    "    output = mean_A * hr_x + mean_b\n",
    "    return output\n",
    "\n",
    "\n",
    "#（2）卡通化函数定义：\n",
    "def cartoonize(load_folder, save_folder, model_path):\n",
    "    input_photo = tf.placeholder(tf.float32, [1, None, None, 3])\n",
    "    network_out = network.unet_generator(input_photo)\n",
    "    final_out = guided_filter.guided_filter(input_photo, network_out, r=1, eps=5e-3)\n",
    "    all_vars = tf.trainable_variables()\n",
    "    gene_vars = [var for var in all_vars if 'generator' in var.name]\n",
    "    saver = tf.train.Saver(var_list=gene_vars)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    name_list = os.listdir(load_folder)\n",
    "    for name in tqdm(name_list):\n",
    "        try:\n",
    "            load_path = os.path.join(load_folder, name)\n",
    "            save_path = os.path.join(save_folder, name)\n",
    "            image = cv2.imread(load_path)\n",
    "            image = resize_crop(image)\n",
    "            batch_image = image.astype(np.float32)/127.5 - 1\n",
    "            batch_image = np.expand_dims(batch_image, axis=0)\n",
    "            output = sess.run(final_out, feed_dict={input_photo: batch_image})\n",
    "            output = (np.squeeze(output)+1)*127.5\n",
    "            output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "            cv2.imwrite(save_path, output)\n",
    "        except:\n",
    "            print('cartoonize {} failed'.format(load_path))\n",
    "\n",
    "            \n",
    "#（3）模型调用\n",
    "model_path = 'saved_models'\n",
    "load_folder = 'test_images'\n",
    "save_folder = 'cartoonized_images'\n",
    "if not os.path.exists(save_folder):\n",
    "        os.mkdir(save_folder)\n",
    "cartoonize(load_folder, save_folder, model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
